{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c00de-cb14-4d07-aaf0-89142fd118b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing...\")\n",
    "from pprint import pprint\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from shutil import rmtree\n",
    "from copy import deepcopy\n",
    "from os.path import join\n",
    "from importlib import reload\n",
    "\n",
    "from os.path import isdir\n",
    "from os.path import join\n",
    "from os import mkdir\n",
    "from shutil import rmtree\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import callbacks\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"../scs\"))\n",
    "import scs_config as scsc\n",
    "import data_loading as dl\n",
    "import data_degrading as dd\n",
    "import data_preparation as dp\n",
    "import data_augmentation as da\n",
    "import data_plotting as dplt\n",
    "import learn\n",
    "import lr_schedules\n",
    "import hp_sets\n",
    "import prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f9f0d-b537-4762-bac6-a306488a7628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SCS(dir_model, R, hp, file_raw_data=None, resume=False):\n",
    "    dir_model_backup = join(dir_model, \"backup\")\n",
    "    dir_model_data = join(dir_model, \"data\")\n",
    "    dir_model_results = join(dir_model, \"results\")\n",
    "\n",
    "    file_model = join(dir_model, \"model.hdf5\")\n",
    "    file_model_history = join(dir_model, \"history.log\")\n",
    "    file_model_results = join(dir_model_results, \"results.json\")\n",
    "    file_model_hp = join(dir_model, \"hp.json\")\n",
    "    file_model_curves = join(dir_model_results, \"curves.pdf\")\n",
    "    \n",
    "    if isfile(file_model_results):\n",
    "        os.remove(file_model_results)\n",
    "        \n",
    "    if isfile(file_model_hp):\n",
    "        os.remove(file_model_hp)\n",
    "    \n",
    "    file_df_trn = join(dir_model_data, \"df_trn.parquet\")\n",
    "    file_df_tst = join(dir_model_data, \"df_tst.parquet\")\n",
    "    if not resume:\n",
    "        df_trn, df_tst = prepare_R_data(\n",
    "            R,\n",
    "            file_raw_data,\n",
    "            phase_range=hp[\"phase_range\"],\n",
    "            ptp_range=hp[\"ptp_range\"],\n",
    "            wvl_range=hp[\"wvl_range\"],\n",
    "            train_frac=hp[\"train_frac\"],\n",
    "            noise_scale=hp[\"noise_scale\"],\n",
    "            spike_scale=hp[\"spike_scale\"],\n",
    "            max_spikes=hp[\"max_spikes\"],\n",
    "            random_state=hp[\"random_state\"],\n",
    "        )\n",
    "        df_trn.to_parquet(file_df_trn)\n",
    "        df_tst.to_parquet(file_df_tst)\n",
    "\n",
    "    df_trn = load_sn_data(file_df_trn)\n",
    "    df_tst = load_sn_data(file_df_tst)\n",
    "\n",
    "    # TODO: Add a function call here to generate some summary statistics\n",
    "    # and/or plots based on df_trn and df_tst.\n",
    "\n",
    "    Xtrn, Ytrn, num_trn, num_wvl, num_classes = extract(df_trn)\n",
    "    Xtst, Ytst, num_tst, num_wvl, num_classes = extract(df_tst)\n",
    "    if hp[\"add_dim\"]:\n",
    "        Xtrn = add_dim(Xtrn, swap=hp[\"swap\"])\n",
    "        Xtst = add_dim(Xtst, swap=hp[\"swap\"])\n",
    "\n",
    "    write_json(hp, file_model_hp)\n",
    "\n",
    "    input_shape = Xtrn.shape[1:]\n",
    "    model = get_model(input_shape, num_classes, hp)\n",
    "    model.summary()\n",
    "\n",
    "    compile_model(model, num_classes, hp[\"lr0\"])\n",
    "    lr_schedule = get_lr_schedule(hp)\n",
    "    callbacks = get_callbacks(dir_model, lr_schedule)\n",
    "\n",
    "    history = train(\n",
    "        model,\n",
    "        Xtrn,\n",
    "        Ytrn,\n",
    "        Xtst,\n",
    "        Ytst,\n",
    "        hp[\"epochs\"],\n",
    "        hp[\"batch_size\"],\n",
    "        callbacks,\n",
    "    )\n",
    "\n",
    "    results = evaluate(model, Xtrn, Ytrn, Xtst, Ytst, verbose=0)\n",
    "    write_json(results, file_model_results)\n",
    "    \n",
    "    log = pd.read_csv(file_model_history)\n",
    "    fig = dplt.plot_loss(log)\n",
    "    fig.savefig(file_model_curves)\n",
    "    fig.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26fa54-1d23-4bc5-b187-c0a18bf2a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 100\n",
    "data_dir_original = \"/home/2649/repos/SCS/data/\"\n",
    "dir_models = \"/lustre/lrspec/users/2649/models/transformer_testing\"\n",
    "\n",
    "# Construct the directories if they don't exist or delete them and recreate\n",
    "# them if they do and `restart_fit` is `True`.\n",
    "dir_model = join(dir_models, f\"{R}_dev\")\n",
    "dir_backup = join(dir_model, \"backup\")\n",
    "dir_model_data = join(dir_model, \"data\")\n",
    "if isdir(dir_model):\n",
    "    rmtree(dir_model)\n",
    "mkdir(dir_model)\n",
    "mkdir(dir_backup)\n",
    "mkdir(dir_model_data)\n",
    "\n",
    "file_trn = join(dir_model_data, f\"sn_data_trn.RPA.parquet\")\n",
    "file_tst = join(dir_model_data, f\"sn_data_tst.RP.parquet\")\n",
    "\n",
    "hp = deepcopy(scsc.default_hyper_parameters)\n",
    "\n",
    "hp[\"train_frac\"] = 0.80\n",
    "hp[\"noise_scale\"] = 0.15848931924611134\n",
    "hp[\"spike_scale\"] = 1.045639552591273\n",
    "hp[\"max_spikes\"] = 3\n",
    "\n",
    "# Prepare the dataset from the original dataset dataframe `sn_data_file`.\n",
    "sn_data_file = join(data_dir_original, \"sn_data.parquet\")\n",
    "prepare_dataset.prepare_dataset(\n",
    "    R,\n",
    "    sn_data_file,\n",
    "    dir_model_data,\n",
    "    dir_model_data,\n",
    "    dir_model_data,\n",
    "    dir_model_data,\n",
    "    hp[\"phase_range\"],\n",
    "    hp[\"ptp_range\"],\n",
    "    hp[\"wvl_range\"],\n",
    "    hp[\"train_frac\"],\n",
    "    hp[\"noise_scale\"],\n",
    "    hp[\"spike_scale\"],\n",
    "    hp[\"max_spikes\"],\n",
    "    random_state=hp[\"random_state\"],\n",
    ")\n",
    "\n",
    "# Load the dataset.\n",
    "df_trn = dl.load_sn_data(file_trn)\n",
    "df_tst = dl.load_sn_data(file_tst)\n",
    "dataset, num_wvl, num_classes = learn.prepare_datasets_for_training(\n",
    "    df_trn, df_tst\n",
    ")\n",
    "Xtrn, Ytrn, Xtst, Ytst = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472d0670-a159-4571-b8fc-50c8b750ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrn.shape, Ytrn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b98b5c-8edc-4d2f-922a-9f75c6aee8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtst.shape, Ytst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07be19-34ec-4cf9-8951-4f8a5b63bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(learn)\n",
    "model = learn.devmodel(\n",
    "    num_wvls=Xtrn.shape[1],\n",
    "    num_classes=num_classes,\n",
    "    num_transformer_blocks=1,\n",
    "    num_heads=4,\n",
    "    key_dim=4,\n",
    "    kr_l2=0,\n",
    "    br_l2=0,\n",
    "    ar_l2=0,\n",
    "    dropout_attention=0,\n",
    "    dropout_projection=0,\n",
    "    filters=512,\n",
    "    num_feed_forward_layers=3,\n",
    "    feed_forward_layer_size=1024,\n",
    "    dropout_feed_forward=0,\n",
    "    initial_projection=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f66f07c-12ea-4be3-8300-0a8b0fd5ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2f9f3-7fef-44a3-97ee-a493cab7bfd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
