{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c05aa4-c4bf-47b5-abe3-9bbb6d7d885e",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Testing out XGBoost and it did not work lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f8c00de-cb14-4d07-aaf0-89142fd118b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing...\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing...\")\n",
    "from pprint import pprint\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from shutil import rmtree\n",
    "from copy import deepcopy\n",
    "from os.path import join\n",
    "from importlib import reload\n",
    "\n",
    "from os.path import isdir\n",
    "from os.path import join\n",
    "from os import mkdir\n",
    "from shutil import rmtree\n",
    "\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"../scs\"))\n",
    "import scs_config as scsc\n",
    "import data_loading as dl\n",
    "import data_degrading as dd\n",
    "import data_preparation as dp\n",
    "import data_augmentation as da\n",
    "import data_plotting as dplt\n",
    "import learn\n",
    "import lr_schedules\n",
    "import hp_sets\n",
    "import prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc26fa54-1d23-4bc5-b187-c0a18bf2a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 100\n",
    "data_dir_original = \"/home/2649/repos/SCS/data/\"\n",
    "dir_models = \"/lustre/lrspec/users/2649/models/transformer_testing\"\n",
    "\n",
    "# Construct the directories if they don't exist or delete them and recreate\n",
    "# them if they do and `restart_fit` is `True`.\n",
    "dir_model = join(dir_models, f\"{R}_xgboost\")\n",
    "dir_backup = join(dir_model, \"backup\")\n",
    "dir_model_data = join(dir_model, \"data\")\n",
    "if isdir(dir_model):\n",
    "    rmtree(dir_model)\n",
    "mkdir(dir_model)\n",
    "mkdir(dir_backup)\n",
    "mkdir(dir_model_data)\n",
    "\n",
    "file_trn = join(dir_model_data, f\"sn_data_trn.RPA.parquet\")\n",
    "file_tst = join(dir_model_data, f\"sn_data_tst.RP.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06321c42-4081-4196-a45c-c0c361195c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = deepcopy(scsc.default_hyper_parameters)\n",
    "\n",
    "hp[\"train_frac\"] = 0.80\n",
    "hp[\"noise_scale\"] = 0.15848931924611134\n",
    "hp[\"spike_scale\"] = 1.045639552591273\n",
    "hp[\"max_spikes\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3f0ab4-9031-4f5a-85d5-707709850020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: /home/2649/repos/SCS/data/sn_data.parquet\n",
      "Artificially degrading the following dataset to a spectral resolution of 100:\n",
      "    /home/2649/repos/SCS/data/sn_data.parquet\n",
      "Saved: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data.C.parquet\n",
      "Saved: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data.R.parquet\n",
      "Degrading completed.\n",
      "\n",
      "Preprocesing the following dataset files:\n",
      "    /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data.C.parquet\n",
      "    /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data.R.parquet\n",
      "Saved: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data.CP.parquet\n",
      "Saved: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data.RP.parquet\n",
      "Preprocessing complete.\n",
      "\n",
      "Performing special train-test split on the following dataset files:\n",
      "    /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data.CP.parquet\n",
      "    /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data.RP.parquet\n",
      "Saved: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data_trn.CP.parquet\n",
      "Saved: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data_tst.CP.parquet\n",
      "Saved: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data_trn.RP.parquet\n",
      "Saved: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data_tst.RP.parquet\n",
      "Train-test split complete.\n",
      "\n",
      "Performing data augmentation on the following dataset files:\n",
      "    /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data_trn.CP.parquet\n",
      "    /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data_tst.CP.parquet\n",
      "Saved: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data_trn.CPA.parquet\n",
      "Saved: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data_trn.RPA.parquet\n",
      "Data augmentation complete.\n",
      "\n",
      "Loaded: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data_trn.RPA.parquet\n",
      "Loaded: /lustre/lrspec/users/2649/models/transformer_testing/100_xgboost/data/sn_data_tst.RP.parquet\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset from the original dataset dataframe `sn_data_file`.\n",
    "sn_data_file = join(data_dir_original, \"sn_data.parquet\")\n",
    "prepare_dataset.prepare_dataset(\n",
    "    R,\n",
    "    sn_data_file,\n",
    "    dir_model_data,\n",
    "    dir_model_data,\n",
    "    dir_model_data,\n",
    "    dir_model_data,\n",
    "    hp[\"phase_range\"],\n",
    "    hp[\"ptp_range\"],\n",
    "    hp[\"wvl_range\"],\n",
    "    hp[\"train_frac\"],\n",
    "    hp[\"noise_scale\"],\n",
    "    hp[\"spike_scale\"],\n",
    "    hp[\"max_spikes\"],\n",
    "    random_state=hp[\"random_state\"],\n",
    ")\n",
    "\n",
    "# Load the dataset.\n",
    "df_trn = dl.load_sn_data(file_trn)\n",
    "df_tst = dl.load_sn_data(file_tst)\n",
    "dataset, num_wvl, num_classes = learn.prepare_datasets_for_training(\n",
    "    df_trn, df_tst\n",
    ")\n",
    "Xtrn, Ytrn, Xtst, Ytst = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd2566b7-b7ba-4f49-b5b0-e3571aa9a8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    # learning_rate=1e-3,\n",
    "    verbosity=1,\n",
    "    objective=\"binary:logistic\"\n",
    ")\n",
    "xgb.fit(Xtrn[..., 0], np.argmax(Ytrn, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6bef2c9-77d1-4eaf-bc9e-6de8684d9953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8394904458598726\n",
      "1.0\n",
      "0.5664886326539337\n"
     ]
    }
   ],
   "source": [
    "print(xgb.score(Xtrn[..., 0], np.argmax(Ytrn, axis=1)))\n",
    "print(xgb.score(Xtst[..., 0], np.argmax(Ytst, axis=1)))\n",
    "\n",
    "Ptrn = xgb.predict(Xtrn[..., 0])\n",
    "Ptst = xgb.predict(Xtst[..., 0])\n",
    "Ptrn.shape, Ptst.shape\n",
    "\n",
    "print(f1_score(np.argmax(Ytrn, axis=1), Ptrn, average=\"macro\"))\n",
    "print(f1_score(np.argmax(Ytst, axis=1), Ptst, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07be19-34ec-4cf9-8951-4f8a5b63bc87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
