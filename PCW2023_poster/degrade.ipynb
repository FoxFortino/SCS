{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647b4c21-b312-4270-a993-02caa08206b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 16:43:17.808583: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing...\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from os import mkdir\n",
    "from os.path import join\n",
    "from os.path import isdir\n",
    "from os.path import isfile\n",
    "from os.path import abspath\n",
    "from shutil import rmtree\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, \"../scs\")\n",
    "import data_degrading as dd\n",
    "import data_preparation as dp\n",
    "import data_augmentation as da\n",
    "from prepare_datasets_for_training import extract\n",
    "from learn import compile_model, get_callbacks, train\n",
    "from lr_schedules import get_lr_schedule\n",
    "import data_plotting as dplt\n",
    "\n",
    "sys.path.insert(0, \"../scs/models\")\n",
    "import feed_forward\n",
    "import transformer_encoder\n",
    "import dash\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2f57fb-8fe6-4076-8508-e227b53c76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 100\n",
    "complete_overwrite = False\n",
    "overwrite_degraded_data = False  # Degraded data should not have to be made more than once per R.\n",
    "\n",
    "# These paremeters are more or less set and I personally should not be tweaking these very much anymore.\n",
    "phase_range = (-20, 50)\n",
    "ptp_range = (0.1, 100)\n",
    "wvl_range = (4500, 7000)\n",
    "\n",
    "train_frac = 0.65\n",
    "noise_scale = 0.25\n",
    "spike_scale = 3.0\n",
    "max_spikes = 5\n",
    "random_state = 1415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b0870d-22a6-42d5-9cf4-910998d8a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_original = \"/home/2649/repos/SCS/data/\"\n",
    "assert isdir(data_dir_original)\n",
    "\n",
    "file_raw_data = join(data_dir_original, \"sn_data.parquet\")\n",
    "assert isfile(file_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a969b09-7493-4c4c-a6a3-0da973281541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in raw data file: /home/2649/repos/SCS/data/sn_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load the raw data\n",
    "print(f\"Reading in raw data file: {abspath(file_raw_data)}\")\n",
    "df_raw = pd.read_parquet(file_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247bad24-c8c6-4b72-867c-eb03ae61d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory that contains all of the degraded spectral data.\n",
    "data_dir_degraded = \"/lustre/lrspec/users/2649/spectralib\"\n",
    "assert isdir(data_dir_degraded)\n",
    "\n",
    "# Directory that contains the parquet files for all of the relevent steps in the data preparation process.\n",
    "data_dir_degraded_R = join(data_dir_degraded, f\"{R}\")\n",
    "if not isdir(data_dir_degraded_R):\n",
    "    print(f\"Creating '{data_dir_degraded_R}'.\")\n",
    "    mkdir(data_dir_degraded_R)\n",
    "assert isdir(data_dir_degraded_R)\n",
    "\n",
    "if complete_overwrite:\n",
    "    print(f\"Deleting and remaking '{data_dir_degraded_R}'.\")\n",
    "    rmtree(data_dir_degraded_R)\n",
    "    mkdir(data_dir_degraded_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3548930-3710-424f-880b-05c810a5b1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data previously degraded to R = 100: '/lustre/lrspec/users/2649/spectralib/100/sn_data.C.parquet' and '/lustre/lrspec/users/2649/spectralib/100/sn_data.R.parquet'.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "file_df_C = join(data_dir_degraded_R, \"sn_data.C.parquet\")\n",
    "file_df_R = join(data_dir_degraded_R, \"sn_data.R.parquet\")\n",
    "\n",
    "if overwrite_degraded_data or (not isfile(file_df_C)) or (not isfile(file_df_R)):\n",
    "    print(f\"Degrading the dataset to R = {R}.\")\n",
    "    df_C, df_R = dd.degrade_dataframe(R, df_raw)\n",
    "\n",
    "    df_C.to_parquet(file_df_C)\n",
    "    df_R.to_parquet(file_df_R)\n",
    "\n",
    "else:\n",
    "    print(f\"Loading data previously degraded to R = {R}: '{file_df_C}' and '{file_df_R}'.\")\n",
    "\n",
    "    assert isfile(file_df_C)\n",
    "    assert isfile(file_df_R)\n",
    "    df_C = pd.read_parquet(file_df_C)\n",
    "    df_R = pd.read_parquet(file_df_R)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64589954-b3ce-4a8a-b421-bec8e8c59e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the dataset.\n",
      "Phase Range (in days): (-20, 50)\n",
      "Peak-to-Peak Range (in spectral units): (0.1, 100)\n",
      "Wavelength Range (in Angstroms): (4500, 7000)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset\n",
    "print(\"Preprocessing the dataset.\")\n",
    "print(f\"Phase Range (in days): {phase_range}\")\n",
    "print(f\"Peak-to-Peak Range (in spectral units): {ptp_range}\")\n",
    "print(f\"Wavelength Range (in Angstroms): {wvl_range}\")\n",
    "\n",
    "df_RP = dp.preproccess_dataframe(\n",
    "    df_R,\n",
    "    phase_range=phase_range,\n",
    "    ptp_range=ptp_range,\n",
    "    wvl_range=wvl_range,\n",
    ")\n",
    "\n",
    "df_CP = dp.preproccess_dataframe(\n",
    "    df_C,\n",
    "    phase_range=phase_range,\n",
    "    ptp_range=ptp_range,\n",
    "    wvl_range=wvl_range,\n",
    ")\n",
    "\n",
    "file_df_CP = join(data_dir_degraded_R, \"sn_data.CP.parquet\")\n",
    "file_df_RP = join(data_dir_degraded_R, \"sn_data.RP.parquet\")\n",
    "df_CP.to_parquet(file_df_CP)\n",
    "df_RP.to_parquet(file_df_RP)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc9b1e57-056c-4039-9253-ba0999434543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform a special train-test split on the dataset.\n",
      "This train-test split splits the dataset by SNe, not by spectra.\n",
      "Fraction of SNe in the training set: 0.65\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Perform the special train-test split\n",
    "print(\"Perform a special train-test split on the dataset.\")\n",
    "print(\"This train-test split splits the dataset by SNe, not by spectra.\")\n",
    "print(f\"Fraction of SNe in the training set: {train_frac}\")\n",
    "rng = np.random.RandomState(random_state)\n",
    "\n",
    "df_RP_trn, df_RP_tst = dp.split_data(df_RP, train_frac, rng)\n",
    "df_CP_trn, df_CP_tst = dp.split_data(df_CP, train_frac, rng)\n",
    "\n",
    "file_df_RP_trn = join(data_dir_degraded_R, \"sn_data_trn.RP.parquet\")\n",
    "file_df_RP_tst = join(data_dir_degraded_R, \"sn_data_tst.RP.parquet\")\n",
    "file_df_CP_trn = join(data_dir_degraded_R, \"sn_data_trn.CP.parquet\")\n",
    "file_df_CP_tst = join(data_dir_degraded_R, \"sn_data_tst.CP.parquet\")\n",
    "\n",
    "df_RP_trn.to_parquet(file_df_RP_trn)\n",
    "df_RP_tst.to_parquet(file_df_RP_tst)\n",
    "df_CP_trn.to_parquet(file_df_CP_trn)\n",
    "df_CP_tst.to_parquet(file_df_CP_tst)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87680e34-627b-4a7b-8745-08cd03d93e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting the dataset.\n",
      "Scale of noise to augment the data with: 0.25\n",
      "Scale of the spikes to augment the data with: 3.0\n",
      "Maximum spikes to augment the dataset with: 5\n",
      "Done.\n",
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "# Augment the training set\n",
    "print(\"Augmenting the dataset.\")\n",
    "print(f\"Scale of noise to augment the data with: {noise_scale}\")\n",
    "print(f\"Scale of the spikes to augment the data with: {spike_scale}\")\n",
    "print(f\"Maximum spikes to augment the dataset with: {max_spikes}\")\n",
    "df_RPA_trn = da.augment(\n",
    "    df_RP_trn,\n",
    "    rng, \n",
    "    wvl_range=wvl_range,\n",
    "    noise_scale=noise_scale,\n",
    "    spike_scale=spike_scale,\n",
    "    max_spikes=max_spikes\n",
    ")\n",
    "\n",
    "df_CPA_trn = da.augment(\n",
    "    df_CP_trn,\n",
    "    rng, \n",
    "    wvl_range=wvl_range,\n",
    "    noise_scale=noise_scale,\n",
    "    spike_scale=spike_scale,\n",
    "    max_spikes=max_spikes\n",
    ")\n",
    "\n",
    "file_df_RPA_trn = join(data_dir_degraded_R, \"sn_data_trn.RPA.parquet\")\n",
    "file_df_CPA_trn = join(data_dir_degraded_R, \"sn_data_trn.CPA.parquet\")\n",
    "df_RPA_trn.to_parquet(file_df_RPA_trn)\n",
    "df_CPA_trn.to_parquet(file_df_CPA_trn)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Data preparation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
